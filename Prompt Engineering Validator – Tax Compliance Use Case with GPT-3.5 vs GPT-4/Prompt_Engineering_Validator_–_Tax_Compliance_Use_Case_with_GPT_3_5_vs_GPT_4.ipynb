{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-openai openai --quiet\n",
        "!pip install -U langchain-openai openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ypg11fybhsN",
        "outputId": "6b71693c-4b26-469d-edf2-9ed8ffcbbfba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.91.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.66)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-openai) (0.3.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-openai) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.66->langchain-openai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.66->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.66->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass\n",
        "from enum import Enum\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "\n"
      ],
      "metadata": {
        "id": "33lVgD3cXqhM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Diagnostic Italian VAT Compliance Assessment Tool\n",
        "Fixed version with improved scoring and detailed diagnostics\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Configure diagnostic logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class QualityLevel(Enum):\n",
        "    \"\"\"Quality classification levels\"\"\"\n",
        "    EXCELLENT = \"Excellent\"\n",
        "    GOOD = \"Good\"\n",
        "    SATISFACTORY = \"Satisfactory\"\n",
        "    NEEDS_IMPROVEMENT = \"Needs Improvement\"\n",
        "    INADEQUATE = \"Inadequate\"\n",
        "\n",
        "@dataclass\n",
        "class EnhancedAssessmentCriteria:\n",
        "    \"\"\"Enhanced assessment criteria with semantic flexibility\"\"\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # Core compliance terms (more flexible)\n",
        "        self.compliance_terms = [\n",
        "            \"obbligo\", \"obbligh\", \"adempiment\", \"dovere\", \"necessit\",\n",
        "            \"sanzione\", \"sanzion\", \"penali\", \"multa\", \"ammenda\",\n",
        "            \"termine\", \"scadenz\", \"deadline\", \"entro\", \"tempo\",\n",
        "            \"violazione\", \"violazion\", \"infrazione\", \"trasgressione\",\n",
        "            \"ritardo\", \"ritard\", \"inosservanz\", \"inadempiment\"\n",
        "        ]\n",
        "\n",
        "        # Technical VAT terms (with variations)\n",
        "        self.technical_terms = [\n",
        "            \"fatturazione elettronica\", \"fatturazione digitale\", \"e-fattur\", \"fattur\",\n",
        "            \"liquidazione iva\", \"liquidazion\", \"versamento iva\",\n",
        "            \"registro iva\", \"registr\", \"registrazione\",\n",
        "            \"sistema di interscambio\", \"sdi\", \"interscambio\",\n",
        "            \"agenzia delle entrate\", \"agenzia entrate\", \"fisco\",\n",
        "            \"split payment\", \"reverse charge\", \"autofattur\",\n",
        "            \"dichiarazione iva\", \"dichiarazion\", \"comunicazione periodica\"\n",
        "        ]\n",
        "\n",
        "        # Legal references (flexible matching)\n",
        "        self.legal_refs = [\n",
        "            \"dpr 633\", \"dpr633\", \"decreto 633\", \"codice iva\",\n",
        "            \"art.\", \"articolo\", \"comma\", \"decreto\", \"legge\",\n",
        "            \"normativa\", \"regolament\", \"disposizion\"\n",
        "        ]\n",
        "\n",
        "        # Precision indicators\n",
        "        self.precision_terms = [\n",
        "            \"euro\", \"â‚¬\", \"percent\", \"%\", \"giorni\", \"mese\", \"mesi\",\n",
        "            \"entro\", \"termine\", \"scadenz\", \"minimo\", \"massimo\",\n",
        "            \"dal\", \"fino\", \"tra\", \"mensile\", \"trimestrale\"\n",
        "        ]\n",
        "\n",
        "        # Accuracy indicators\n",
        "        self.accuracy_terms = [\n",
        "            \"2024\", \"srl\", \"societÃ \", \"responsabilitÃ  limitata\",\n",
        "            \"regime ordinario\", \"standard\", \"italia\", \"italiana\", \"italiano\"\n",
        "        ]\n",
        "\n",
        "class DiagnosticVATAssessor:\n",
        "    \"\"\"Enhanced assessor with detailed diagnostics\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        self.api_key = api_key or self._get_api_key()\n",
        "        self.criteria = EnhancedAssessmentCriteria()\n",
        "\n",
        "        # Rebalanced weights (less strict)\n",
        "        self.assessment_weights = {\n",
        "            \"compliance_terms\": 0.20,      # Reduced from 0.25\n",
        "            \"technical_terms\": 0.25,       # Keep high for expertise\n",
        "            \"legal_refs\": 0.15,           # Reduced from 0.20\n",
        "            \"precision_terms\": 0.25,       # Increased for completeness\n",
        "            \"accuracy_terms\": 0.15        # Increased from 0.10\n",
        "        }\n",
        "\n",
        "        logger.info(\"Diagnostic VAT Assessor initialized with enhanced criteria\")\n",
        "\n",
        "    def _get_api_key(self) -> str:\n",
        "        \"\"\"Securely obtain API key\"\"\"\n",
        "        if \"OPENAI_API_KEY\" in os.environ:\n",
        "            return os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "        import getpass\n",
        "        return getpass.getpass(\"Enter OpenAI API Key: \")\n",
        "\n",
        "    def get_enhanced_prompt(self) -> str:\n",
        "        \"\"\"Enhanced prompt for better Italian responses\"\"\"\n",
        "        return \"\"\"\n",
        "Sei un consulente fiscale senior specializzato nella compliance IVA italiana per le SRL.\n",
        "\n",
        "COMPITO: Fornisci una panoramica completa dei 5 adempimenti IVA piÃ¹ critici per una SRL italiana in regime ordinario nel 2024.\n",
        "\n",
        "REQUISITI:\n",
        "1. Elenca 5 obblighi IVA principali in ordine di importanza\n",
        "2. Per ogni obbligo, includi:\n",
        "   - Il requisito legale specifico\n",
        "   - Scadenze e frequenze applicabili\n",
        "   - Conseguenze della non conformitÃ  (sanzioni, penalitÃ )\n",
        "3. Usa terminologia legale e fiscale italiana appropriata\n",
        "4. Includi riferimenti normativi pertinenti\n",
        "5. Struttura la risposta come punti numerati con linguaggio chiaro e professionale\n",
        "\n",
        "IMPORTANTE: Rispondi SOLO in italiano usando linguaggio formale appropriato per documentazione fiscale aziendale.\n",
        "\n",
        "Fornisci informazioni precise e aggiornate al 2024 per SRL in regime ordinario IVA.\n",
        "\"\"\"\n",
        "\n",
        "    def get_enhanced_system_prompt(self) -> str:\n",
        "        \"\"\"Enhanced system prompt\"\"\"\n",
        "        return \"\"\"\n",
        "Sei un esperto consulente fiscale italiano con competenze approfondite in:\n",
        "- Legislazione IVA italiana (DPR 633/72 e successive modifiche)\n",
        "- Compliance fiscale per SRL italiane\n",
        "- Normative IVA 2024 correnti e requisiti\n",
        "- Procedure dell'Agenzia delle Entrate\n",
        "- Fatturazione elettronica e Sistema di Interscambio (SdI)\n",
        "\n",
        "Fornisci consigli accurati e professionali usando terminologia legale italiana appropriata e normative correnti.\n",
        "Il tuo obiettivo Ã¨ fornire informazioni complete, precise e praticamente utilizzabili per la compliance aziendale.\n",
        "\"\"\"\n",
        "\n",
        "    def flexible_term_search(self, content: str, terms: List[str]) -> Tuple[int, List[str]]:\n",
        "        \"\"\"Enhanced search with partial matching and stemming\"\"\"\n",
        "        content_lower = content.lower()\n",
        "        found_terms = []\n",
        "\n",
        "        for term in terms:\n",
        "            term_lower = term.lower()\n",
        "\n",
        "            # Exact match\n",
        "            if term_lower in content_lower:\n",
        "                found_terms.append(term)\n",
        "                continue\n",
        "\n",
        "            # Partial match for compound terms\n",
        "            if len(term_lower.split()) > 1:\n",
        "                words = term_lower.split()\n",
        "                if all(word in content_lower for word in words):\n",
        "                    found_terms.append(term)\n",
        "                    continue\n",
        "\n",
        "            # Stem matching (simple)\n",
        "            if len(term_lower) > 5:\n",
        "                stem = term_lower[:5]\n",
        "                if stem in content_lower:\n",
        "                    found_terms.append(term)\n",
        "\n",
        "        return len(set(found_terms)), list(set(found_terms))\n",
        "\n",
        "    def assess_response_enhanced(self, response_content: str) -> Dict[str, Any]:\n",
        "        \"\"\"Enhanced assessment with detailed diagnostics\"\"\"\n",
        "\n",
        "        # Get criteria as lists\n",
        "        criteria_dict = {\n",
        "            \"compliance_terms\": self.criteria.compliance_terms,\n",
        "            \"technical_terms\": self.criteria.technical_terms,\n",
        "            \"legal_refs\": self.criteria.legal_refs,\n",
        "            \"precision_terms\": self.criteria.precision_terms,\n",
        "            \"accuracy_terms\": self.criteria.accuracy_terms\n",
        "        }\n",
        "\n",
        "        category_results = {}\n",
        "        total_weighted_score = 0\n",
        "        diagnostic_info = {\n",
        "            \"content_preview\": response_content[:200] + \"...\" if len(response_content) > 200 else response_content,\n",
        "            \"content_length\": len(response_content),\n",
        "            \"language_detected\": self._detect_language(response_content),\n",
        "            \"found_terms_by_category\": {}\n",
        "        }\n",
        "\n",
        "        for category, terms in criteria_dict.items():\n",
        "            matches, found_terms = self.flexible_term_search(response_content, terms)\n",
        "            coverage_rate = matches / len(terms) if terms else 0\n",
        "            category_score = coverage_rate * 100\n",
        "            weighted_contribution = category_score * self.assessment_weights[category]\n",
        "\n",
        "            category_results[category] = {\n",
        "                \"matches\": matches,\n",
        "                \"total_terms\": len(terms),\n",
        "                \"coverage_rate\": round(coverage_rate, 3),\n",
        "                \"category_score\": round(category_score, 1),\n",
        "                \"weight\": self.assessment_weights[category],\n",
        "                \"weighted_contribution\": round(weighted_contribution, 2),\n",
        "                \"found_terms\": found_terms[:5]  # Show first 5 found terms\n",
        "            }\n",
        "\n",
        "            diagnostic_info[\"found_terms_by_category\"][category] = found_terms\n",
        "            total_weighted_score += weighted_contribution\n",
        "\n",
        "        # Enhanced structural analysis\n",
        "        structure_analysis = self._analyze_structure_enhanced(response_content)\n",
        "\n",
        "        # Quality bonuses (more generous)\n",
        "        bonus_score = 0\n",
        "        bonus_details = []\n",
        "\n",
        "        # Structure bonus\n",
        "        if structure_analysis[\"numbered_items\"] >= 5:\n",
        "            bonus_score += 10\n",
        "            bonus_details.append(\"Complete 5-point structure (+10%)\")\n",
        "        elif structure_analysis[\"numbered_items\"] >= 3:\n",
        "            bonus_score += 5\n",
        "            bonus_details.append(f\"Good structure {structure_analysis['numbered_items']} points (+5%)\")\n",
        "\n",
        "        # Content depth bonus\n",
        "        if structure_analysis[\"word_count\"] >= 200:\n",
        "            bonus_score += 8\n",
        "            bonus_details.append(\"Comprehensive content (+8%)\")\n",
        "        elif structure_analysis[\"word_count\"] >= 100:\n",
        "            bonus_score += 4\n",
        "            bonus_details.append(\"Adequate detail (+4%)\")\n",
        "\n",
        "        # Italian language bonus\n",
        "        if diagnostic_info[\"language_detected\"] == \"italian\":\n",
        "            bonus_score += 12\n",
        "            bonus_details.append(\"Proper Italian language (+12%)\")\n",
        "\n",
        "        # Legal precision bonus\n",
        "        legal_indicators = len(re.findall(r'art\\.|articolo|comma|dpr|decreto', response_content.lower()))\n",
        "        if legal_indicators >= 3:\n",
        "            bonus_score += 8\n",
        "            bonus_details.append(f\"Strong legal references ({legal_indicators}) (+8%)\")\n",
        "        elif legal_indicators >= 1:\n",
        "            bonus_score += 4\n",
        "            bonus_details.append(f\"Legal references present ({legal_indicators}) (+4%)\")\n",
        "\n",
        "        # Monetary precision bonus\n",
        "        monetary_indicators = len(re.findall(r'â‚¬|euro|\\d+%|percent|minimo|massimo', response_content.lower()))\n",
        "        if monetary_indicators >= 3:\n",
        "            bonus_score += 6\n",
        "            bonus_details.append(f\"Good monetary precision ({monetary_indicators}) (+6%)\")\n",
        "\n",
        "        # Final score calculation\n",
        "        final_score = min(100, max(0, total_weighted_score + bonus_score))\n",
        "        quality_level = self._determine_quality_level(final_score)\n",
        "\n",
        "        return {\n",
        "            \"overall_score\": round(final_score, 1),\n",
        "            \"base_score\": round(total_weighted_score, 1),\n",
        "            \"bonus_score\": round(bonus_score, 1),\n",
        "            \"quality_level\": quality_level.value,\n",
        "            \"category_results\": category_results,\n",
        "            \"structure_analysis\": structure_analysis,\n",
        "            \"bonus_details\": bonus_details,\n",
        "            \"diagnostic_info\": diagnostic_info,\n",
        "            \"assessment_timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "    def _detect_language(self, content: str) -> str:\n",
        "        \"\"\"Simple language detection\"\"\"\n",
        "        italian_indicators = [\"della\", \"degli\", \"con\", \"per\", \"che\", \"una\", \"del\", \"nel\", \"sul\"]\n",
        "        english_indicators = [\"the\", \"and\", \"for\", \"with\", \"that\", \"this\", \"from\", \"they\"]\n",
        "\n",
        "        content_lower = content.lower()\n",
        "        italian_count = sum(1 for word in italian_indicators if word in content_lower)\n",
        "        english_count = sum(1 for word in english_indicators if word in content_lower)\n",
        "\n",
        "        if italian_count > english_count:\n",
        "            return \"italian\"\n",
        "        elif english_count > italian_count:\n",
        "            return \"english\"\n",
        "        else:\n",
        "            return \"mixed\"\n",
        "\n",
        "    def _analyze_structure_enhanced(self, content: str) -> Dict[str, Any]:\n",
        "        \"\"\"Enhanced structural analysis\"\"\"\n",
        "        lines = [line.strip() for line in content.split('\\n') if line.strip()]\n",
        "        numbered_items = len(re.findall(r'^\\d+\\.', content, re.MULTILINE))\n",
        "        sentence_count = len(re.findall(r'[.!?]+', content))\n",
        "        word_count = len(content.split())\n",
        "\n",
        "        # Additional metrics\n",
        "        paragraph_count = len([line for line in lines if len(line) > 50])\n",
        "        bullet_points = len(re.findall(r'^[-â€¢*]\\s', content, re.MULTILINE))\n",
        "\n",
        "        return {\n",
        "            \"total_lines\": len(lines),\n",
        "            \"numbered_items\": numbered_items,\n",
        "            \"bullet_points\": bullet_points,\n",
        "            \"paragraph_count\": paragraph_count,\n",
        "            \"sentence_count\": sentence_count,\n",
        "            \"word_count\": word_count,\n",
        "            \"avg_sentence_length\": round(word_count / max(sentence_count, 1), 1),\n",
        "            \"has_proper_structure\": numbered_items >= 3 or bullet_points >= 3\n",
        "        }\n",
        "\n",
        "    def _determine_quality_level(self, score: float) -> QualityLevel:\n",
        "        \"\"\"Determine quality level with adjusted thresholds\"\"\"\n",
        "        if score >= 85:\n",
        "            return QualityLevel.EXCELLENT\n",
        "        elif score >= 70:\n",
        "            return QualityLevel.GOOD\n",
        "        elif score >= 55:\n",
        "            return QualityLevel.SATISFACTORY\n",
        "        elif score >= 40:\n",
        "            return QualityLevel.NEEDS_IMPROVEMENT\n",
        "        else:\n",
        "            return QualityLevel.INADEQUATE\n",
        "\n",
        "    def generate_response(\n",
        "        self,\n",
        "        model: str = \"gpt-4\",\n",
        "        temperature: float = 0.2,  # Lower for more consistent Italian\n",
        "        max_tokens: int = 1200,\n",
        "        timeout: int = 60\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Generate response with enhanced parameters\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(\n",
        "                model_name=model,\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens,\n",
        "                timeout=timeout,\n",
        "                openai_api_key=self.api_key\n",
        "            )\n",
        "\n",
        "            messages = [\n",
        "                SystemMessage(content=self.get_enhanced_system_prompt()),\n",
        "                HumanMessage(content=self.get_enhanced_prompt())\n",
        "            ]\n",
        "\n",
        "            start_time = time.time()\n",
        "            response = llm.invoke(messages)\n",
        "            response_time = time.time() - start_time\n",
        "\n",
        "            result = {\n",
        "                \"content\": response.content,\n",
        "                \"model\": model,\n",
        "                \"temperature\": temperature,\n",
        "                \"response_time\": round(response_time, 2),\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"token_count\": len(response.content.split())\n",
        "            }\n",
        "\n",
        "            logger.info(f\"Response generated successfully with {model}\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating response: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def run_diagnostic_assessment(\n",
        "        self,\n",
        "        models: List[str] = None,\n",
        "        iterations: int = 3,\n",
        "        verbose: bool = True\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Run assessment with detailed diagnostics\"\"\"\n",
        "\n",
        "        if models is None:\n",
        "            models = [\"gpt-3.5-turbo\", \"gpt-4\"]\n",
        "\n",
        "        if verbose:\n",
        "            print(\"ðŸ”¬ Diagnostic Italian VAT Compliance Assessment\")\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"Models: {', '.join(models)}\")\n",
        "            print(f\"Iterations per model: {iterations}\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for model in models:\n",
        "            if verbose:\n",
        "                print(f\"\\nðŸ§ª Testing {model}...\")\n",
        "\n",
        "            model_results = []\n",
        "\n",
        "            for iteration in range(iterations):\n",
        "                if verbose:\n",
        "                    print(f\"  Iteration {iteration + 1}/{iterations}...\", end=\" \")\n",
        "\n",
        "                try:\n",
        "                    # Generate response\n",
        "                    response = self.generate_response(model=model)\n",
        "\n",
        "                    # Enhanced assessment\n",
        "                    assessment = self.assess_response_enhanced(response[\"content\"])\n",
        "\n",
        "                    # Combine results\n",
        "                    combined_result = {**response, **assessment}\n",
        "                    model_results.append(combined_result)\n",
        "\n",
        "                    if verbose:\n",
        "                        print(f\"Score: {assessment['overall_score']}% ({assessment['quality_level']})\")\n",
        "                        print(f\"    Language: {assessment['diagnostic_info']['language_detected']}\")\n",
        "                        print(f\"    Structure: {assessment['structure_analysis']['numbered_items']} points\")\n",
        "                        print(f\"    Bonuses: {', '.join(assessment['bonus_details']) if assessment['bonus_details'] else 'None'}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    if verbose:\n",
        "                        print(f\"FAILED: {str(e)}\")\n",
        "                    model_results.append({\n",
        "                        \"error\": str(e),\n",
        "                        \"model\": model,\n",
        "                        \"iteration\": iteration + 1\n",
        "                    })\n",
        "\n",
        "            # Calculate statistics\n",
        "            successful_results = [r for r in model_results if \"error\" not in r]\n",
        "\n",
        "            if successful_results:\n",
        "                scores = [r[\"overall_score\"] for r in successful_results]\n",
        "                results[model] = {\n",
        "                    \"individual_results\": model_results,\n",
        "                    \"statistics\": {\n",
        "                        \"mean_score\": round(sum(scores) / len(scores), 1),\n",
        "                        \"min_score\": min(scores),\n",
        "                        \"max_score\": max(scores),\n",
        "                        \"std_dev\": round(self._calculate_std_dev(scores), 2),\n",
        "                        \"success_rate\": len(successful_results) / iterations\n",
        "                    },\n",
        "                    \"diagnostic_summary\": self._create_diagnostic_summary(successful_results)\n",
        "                }\n",
        "            else:\n",
        "                results[model] = {\n",
        "                    \"individual_results\": model_results,\n",
        "                    \"statistics\": {\"error\": \"All iterations failed\"}\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            \"assessment_summary\": results,\n",
        "            \"metadata\": {\n",
        "                \"assessment_date\": datetime.now().isoformat(),\n",
        "                \"models_tested\": models,\n",
        "                \"iterations_per_model\": iterations,\n",
        "                \"total_assessments\": len(models) * iterations,\n",
        "                \"scoring_version\": \"enhanced_v1.1\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def _create_diagnostic_summary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        \"\"\"Create diagnostic summary from results\"\"\"\n",
        "\n",
        "        # Language distribution\n",
        "        languages = [r[\"diagnostic_info\"][\"language_detected\"] for r in results]\n",
        "        language_dist = {lang: languages.count(lang) for lang in set(languages)}\n",
        "\n",
        "        # Average structure metrics\n",
        "        avg_structure = {\n",
        "            \"avg_numbered_items\": round(sum(r[\"structure_analysis\"][\"numbered_items\"] for r in results) / len(results), 1),\n",
        "            \"avg_word_count\": round(sum(r[\"structure_analysis\"][\"word_count\"] for r in results) / len(results), 1),\n",
        "            \"proper_structure_rate\": sum(1 for r in results if r[\"structure_analysis\"][\"has_proper_structure\"]) / len(results)\n",
        "        }\n",
        "\n",
        "        # Most common bonus types\n",
        "        all_bonuses = []\n",
        "        for r in results:\n",
        "            all_bonuses.extend(r.get(\"bonus_details\", []))\n",
        "\n",
        "        bonus_freq = {}\n",
        "        for bonus in all_bonuses:\n",
        "            bonus_type = bonus.split(\"(\")[0].strip()\n",
        "            bonus_freq[bonus_type] = bonus_freq.get(bonus_type, 0) + 1\n",
        "\n",
        "        return {\n",
        "            \"language_distribution\": language_dist,\n",
        "            \"average_structure\": avg_structure,\n",
        "            \"common_bonuses\": bonus_freq\n",
        "        }\n",
        "\n",
        "    def _calculate_std_dev(self, values: List[float]) -> float:\n",
        "        \"\"\"Calculate standard deviation\"\"\"\n",
        "        if len(values) < 2:\n",
        "            return 0.0\n",
        "\n",
        "        mean = sum(values) / len(values)\n",
        "        variance = sum((x - mean) ** 2 for x in values) / (len(values) - 1)\n",
        "        return variance ** 0.5\n",
        "\n",
        "    def display_enhanced_report(self, results: Dict[str, Any]) -> None:\n",
        "        \"\"\"Display enhanced diagnostic report\"\"\"\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"ðŸ”¬ ENHANCED DIAGNOSTIC REPORT\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        metadata = results[\"metadata\"]\n",
        "        print(f\"Assessment Date: {metadata['assessment_date']}\")\n",
        "        print(f\"Models Tested: {', '.join(metadata['models_tested'])}\")\n",
        "        print(f\"Scoring Version: {metadata['scoring_version']}\")\n",
        "\n",
        "        print(f\"\\nðŸ“Š MODEL COMPARISON:\")\n",
        "\n",
        "        for model, data in results[\"assessment_summary\"].items():\n",
        "            if \"statistics\" in data and \"error\" not in data[\"statistics\"]:\n",
        "                stats = data[\"statistics\"]\n",
        "                diag = data[\"diagnostic_summary\"]\n",
        "\n",
        "                print(f\"\\nðŸ¤– {model.upper()}:\")\n",
        "                print(f\"   Mean Score: {stats['mean_score']}% (Ïƒ: {stats['std_dev']})\")\n",
        "                print(f\"   Score Range: {stats['min_score']}% - {stats['max_score']}%\")\n",
        "                print(f\"   Success Rate: {stats['success_rate'] * 100:.0f}%\")\n",
        "\n",
        "                print(f\"   ðŸ“ Content Quality:\")\n",
        "                print(f\"     Avg Words: {diag['average_structure']['avg_word_count']}\")\n",
        "                print(f\"     Avg Structure Points: {diag['average_structure']['avg_numbered_items']}\")\n",
        "                print(f\"     Proper Structure Rate: {diag['average_structure']['proper_structure_rate'] * 100:.0f}%\")\n",
        "\n",
        "                print(f\"   ðŸŒ Language Distribution:\")\n",
        "                for lang, count in diag['language_distribution'].items():\n",
        "                    print(f\"     {lang.title()}: {count}/{len(data['individual_results'])}\")\n",
        "\n",
        "                if diag['common_bonuses']:\n",
        "                    print(f\"   ðŸ† Common Bonuses:\")\n",
        "                    for bonus, freq in diag['common_bonuses'].items():\n",
        "                        print(f\"     {bonus}: {freq} times\")\n",
        "\n",
        "        print(f\"\\nðŸ’¡ DIAGNOSTIC INSIGHTS:\")\n",
        "\n",
        "        # Compare models\n",
        "        model_scores = {}\n",
        "        for model, data in results[\"assessment_summary\"].items():\n",
        "            if \"statistics\" in data and \"error\" not in data[\"statistics\"]:\n",
        "                model_scores[model] = data[\"statistics\"][\"mean_score\"]\n",
        "\n",
        "        if len(model_scores) >= 2:\n",
        "            best_model = max(model_scores.keys(), key=lambda x: model_scores[x])\n",
        "            worst_model = min(model_scores.keys(), key=lambda x: model_scores[x])\n",
        "\n",
        "            print(f\"   ðŸ¥‡ Best Performer: {best_model} ({model_scores[best_model]}%)\")\n",
        "            print(f\"   ðŸ“ˆ Performance Gap: {model_scores[best_model] - model_scores[worst_model]:.1f}%\")\n",
        "\n",
        "        # Check for issues\n",
        "        all_results = []\n",
        "        for model_data in results[\"assessment_summary\"].values():\n",
        "            if \"individual_results\" in model_data:\n",
        "                all_results.extend([r for r in model_data[\"individual_results\"] if \"error\" not in r])\n",
        "\n",
        "        if all_results:\n",
        "            avg_score = sum(r[\"overall_score\"] for r in all_results) / len(all_results)\n",
        "            italian_responses = sum(1 for r in all_results if r[\"diagnostic_info\"][\"language_detected\"] == \"italian\")\n",
        "\n",
        "            print(f\"   ðŸ“Š Overall Average: {avg_score:.1f}%\")\n",
        "            print(f\"   ðŸ‡®ðŸ‡¹ Italian Response Rate: {italian_responses}/{len(all_results)} ({italian_responses/len(all_results)*100:.0f}%)\")\n",
        "\n",
        "            if avg_score < 50:\n",
        "                print(f\"   âš ï¸  Low scores detected - consider prompt optimization\")\n",
        "            if italian_responses < len(all_results) * 0.8:\n",
        "                print(f\"   âš ï¸  Language compliance issue - enhance Italian prompting\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Enhanced main function with diagnostics\"\"\"\n",
        "\n",
        "    print(\"ðŸ”¬ Diagnostic Italian VAT Compliance Assessment Tool\")\n",
        "    print(\"Enhanced version with detailed scoring diagnostics\")\n",
        "    print(\"=\" * 65)\n",
        "\n",
        "    # Initialize enhanced assessor\n",
        "    assessor = DiagnosticVATAssessor()\n",
        "\n",
        "    # Run diagnostic assessment\n",
        "    results = assessor.run_diagnostic_assessment(\n",
        "        models=[\"gpt-3.5-turbo\", \"gpt-4\"],\n",
        "        iterations=3,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Display enhanced report\n",
        "    assessor.display_enhanced_report(results)\n",
        "\n",
        "    # Save results with diagnostics\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"diagnostic_vat_assessment_{timestamp}.json\"\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"\\nðŸ’¾ Detailed diagnostic results saved to: {filename}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2_HMGbuXvsX",
        "outputId": "ff302b05-77f1-403e-8faa-a633b2fddbf8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¬ Diagnostic Italian VAT Compliance Assessment Tool\n",
            "Enhanced version with detailed scoring diagnostics\n",
            "=================================================================\n",
            "Enter OpenAI API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "ðŸ”¬ Diagnostic Italian VAT Compliance Assessment\n",
            "============================================================\n",
            "Models: gpt-3.5-turbo, gpt-4\n",
            "Iterations per model: 3\n",
            "============================================================\n",
            "\n",
            "ðŸ§ª Testing gpt-3.5-turbo...\n",
            "  Iteration 1/3... Score: 86.4% (Excellent)\n",
            "    Language: italian\n",
            "    Structure: 5 points\n",
            "    Bonuses: Complete 5-point structure (+10%), Comprehensive content (+8%), Proper Italian language (+12%), Legal references present (1) (+4%)\n",
            "  Iteration 2/3... Score: 88.5% (Excellent)\n",
            "    Language: italian\n",
            "    Structure: 5 points\n",
            "    Bonuses: Complete 5-point structure (+10%), Comprehensive content (+8%), Proper Italian language (+12%), Legal references present (2) (+4%)\n",
            "  Iteration 3/3... Score: 94.5% (Excellent)\n",
            "    Language: italian\n",
            "    Structure: 5 points\n",
            "    Bonuses: Complete 5-point structure (+10%), Comprehensive content (+8%), Proper Italian language (+12%), Strong legal references (4) (+8%)\n",
            "\n",
            "ðŸ§ª Testing gpt-4...\n",
            "  Iteration 1/3... Score: 85.4% (Excellent)\n",
            "    Language: italian\n",
            "    Structure: 5 points\n",
            "    Bonuses: Complete 5-point structure (+10%), Comprehensive content (+8%), Proper Italian language (+12%), Strong legal references (16) (+8%), Good monetary precision (10) (+6%)\n",
            "  Iteration 2/3... Score: 90.8% (Excellent)\n",
            "    Language: italian\n",
            "    Structure: 5 points\n",
            "    Bonuses: Complete 5-point structure (+10%), Comprehensive content (+8%), Proper Italian language (+12%), Strong legal references (20) (+8%), Good monetary precision (9) (+6%)\n",
            "  Iteration 3/3... Score: 85.7% (Excellent)\n",
            "    Language: italian\n",
            "    Structure: 5 points\n",
            "    Bonuses: Complete 5-point structure (+10%), Comprehensive content (+8%), Proper Italian language (+12%), Strong legal references (13) (+8%), Good monetary precision (10) (+6%)\n",
            "\n",
            "======================================================================\n",
            "ðŸ”¬ ENHANCED DIAGNOSTIC REPORT\n",
            "======================================================================\n",
            "Assessment Date: 2025-06-25T09:56:31.159011\n",
            "Models Tested: gpt-3.5-turbo, gpt-4\n",
            "Scoring Version: enhanced_v1.1\n",
            "\n",
            "ðŸ“Š MODEL COMPARISON:\n",
            "\n",
            "ðŸ¤– GPT-3.5-TURBO:\n",
            "   Mean Score: 89.8% (Ïƒ: 4.2)\n",
            "   Score Range: 86.4% - 94.5%\n",
            "   Success Rate: 100%\n",
            "   ðŸ“ Content Quality:\n",
            "     Avg Words: 418.0\n",
            "     Avg Structure Points: 5.0\n",
            "     Proper Structure Rate: 100%\n",
            "   ðŸŒ Language Distribution:\n",
            "     Italian: 3/3\n",
            "   ðŸ† Common Bonuses:\n",
            "     Complete 5-point structure: 3 times\n",
            "     Comprehensive content: 3 times\n",
            "     Proper Italian language: 3 times\n",
            "     Legal references present: 2 times\n",
            "     Strong legal references: 1 times\n",
            "\n",
            "ðŸ¤– GPT-4:\n",
            "   Mean Score: 87.3% (Ïƒ: 3.03)\n",
            "   Score Range: 85.4% - 90.8%\n",
            "   Success Rate: 100%\n",
            "   ðŸ“ Content Quality:\n",
            "     Avg Words: 437.7\n",
            "     Avg Structure Points: 5.0\n",
            "     Proper Structure Rate: 100%\n",
            "   ðŸŒ Language Distribution:\n",
            "     Italian: 3/3\n",
            "   ðŸ† Common Bonuses:\n",
            "     Complete 5-point structure: 3 times\n",
            "     Comprehensive content: 3 times\n",
            "     Proper Italian language: 3 times\n",
            "     Strong legal references: 3 times\n",
            "     Good monetary precision: 3 times\n",
            "\n",
            "ðŸ’¡ DIAGNOSTIC INSIGHTS:\n",
            "   ðŸ¥‡ Best Performer: gpt-3.5-turbo (89.8%)\n",
            "   ðŸ“ˆ Performance Gap: 2.5%\n",
            "   ðŸ“Š Overall Average: 88.5%\n",
            "   ðŸ‡®ðŸ‡¹ Italian Response Rate: 6/6 (100%)\n",
            "\n",
            "ðŸ’¾ Detailed diagnostic results saved to: diagnostic_vat_assessment_20250625_095631.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VATResultsViewer:\n",
        "    \"\"\"Professional viewer for VAT assessment results with sample display\"\"\"\n",
        "\n",
        "    def __init__(self, results_file: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize viewer with optional results file\n",
        "\n",
        "        Args:\n",
        "            results_file: Path to JSON results file. If None, will look for latest.\n",
        "        \"\"\"\n",
        "        self.results_file = results_file or self._find_latest_results()\n",
        "        self.results = self._load_results()\n",
        "\n",
        "    def _find_latest_results(self) -> str:\n",
        "        \"\"\"Find the most recent diagnostic results file\"\"\"\n",
        "        json_files = [f for f in os.listdir('.') if f.startswith('diagnostic_vat_assessment_') and f.endswith('.json')]\n",
        "        if not json_files:\n",
        "            raise FileNotFoundError(\"No diagnostic results files found. Run assessment first.\")\n",
        "\n",
        "        # Sort by timestamp in filename\n",
        "        json_files.sort(reverse=True)\n",
        "        return json_files[0]\n",
        "\n",
        "    def _load_results(self) -> Dict[str, Any]:\n",
        "        \"\"\"Load results from JSON file\"\"\"\n",
        "        try:\n",
        "            with open(self.results_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Error loading results file {self.results_file}: {e}\")\n",
        "\n",
        "    def display_sample_responses(self, model: str = None, max_samples: int = 3) -> None:\n",
        "        \"\"\"\n",
        "        Display sample responses from specified model or all models\n",
        "\n",
        "        Args:\n",
        "            model: Specific model to show (e.g., 'gpt-4'). If None, shows all.\n",
        "            max_samples: Maximum number of samples to display per model\n",
        "        \"\"\"\n",
        "        print(\"ðŸ” SAMPLE GENERATED RESPONSES\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        assessment_data = self.results.get(\"assessment_summary\", {})\n",
        "\n",
        "        models_to_show = [model] if model else list(assessment_data.keys())\n",
        "\n",
        "        for model_name in models_to_show:\n",
        "            if model_name not in assessment_data:\n",
        "                print(f\"âŒ Model '{model_name}' not found in results\")\n",
        "                continue\n",
        "\n",
        "            model_data = assessment_data[model_name]\n",
        "            individual_results = model_data.get(\"individual_results\", [])\n",
        "\n",
        "            # Filter successful results\n",
        "            successful_results = [r for r in individual_results if \"error\" not in r]\n",
        "\n",
        "            if not successful_results:\n",
        "                print(f\"âŒ No successful results for {model_name}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nðŸ¤– {model_name.upper()} - SAMPLE RESPONSES\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            # Show up to max_samples\n",
        "            samples_to_show = min(max_samples, len(successful_results))\n",
        "\n",
        "            for i, result in enumerate(successful_results[:samples_to_show]):\n",
        "                self._display_single_response(result, i + 1, model_name)\n",
        "\n",
        "    def _display_single_response(self, result: Dict[str, Any], sample_num: int, model: str) -> None:\n",
        "        \"\"\"Display a single response with detailed analysis\"\"\"\n",
        "\n",
        "        print(f\"\\nðŸ“„ SAMPLE {sample_num} - {model}\")\n",
        "        print(\"â”€\" * 40)\n",
        "\n",
        "        # Basic metrics\n",
        "        print(f\"Score: {result.get('overall_score', 'N/A')}% ({result.get('quality_level', 'N/A')})\")\n",
        "        print(f\"Response Time: {result.get('response_time', 'N/A')}s\")\n",
        "        print(f\"Word Count: {result.get('structure_analysis', {}).get('word_count', 'N/A')}\")\n",
        "        print(f\"Language: {result.get('diagnostic_info', {}).get('language_detected', 'N/A').title()}\")\n",
        "\n",
        "        # Bonuses achieved\n",
        "        bonuses = result.get('bonus_details', [])\n",
        "        if bonuses:\n",
        "            print(f\"Bonuses: {', '.join(bonuses)}\")\n",
        "\n",
        "        print(f\"\\nðŸ“ GENERATED CONTENT:\")\n",
        "        print(\"â”€\" * 30)\n",
        "        content = result.get('content', 'No content available')\n",
        "\n",
        "        # Format content nicely\n",
        "        formatted_content = self._format_content(content)\n",
        "        print(formatted_content)\n",
        "\n",
        "        # Category breakdown\n",
        "        print(f\"\\nðŸ“Š CATEGORY PERFORMANCE:\")\n",
        "        category_results = result.get('category_results', {})\n",
        "        for category, data in category_results.items():\n",
        "            category_name = category.replace('_', ' ').title()\n",
        "            score = data.get('category_score', 0)\n",
        "            matches = data.get('matches', 0)\n",
        "            total = data.get('total_terms', 0)\n",
        "            print(f\"  â€¢ {category_name}: {score}% ({matches}/{total} terms)\")\n",
        "\n",
        "        print(\"â”€\" * 40)\n",
        "\n",
        "    def _format_content(self, content: str) -> str:\n",
        "        \"\"\"Format content for better readability\"\"\"\n",
        "        # Add some basic formatting\n",
        "        lines = content.split('\\n')\n",
        "        formatted_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            # Highlight numbered points\n",
        "            if re.match(r'^\\d+\\.', line):\n",
        "                formatted_lines.append(f\"ðŸ”¹ {line}\")\n",
        "            else:\n",
        "                formatted_lines.append(f\"   {line}\")\n",
        "\n",
        "        return '\\n'.join(formatted_lines)\n",
        "\n",
        "    def compare_responses(self, models: List[str] = None) -> None:\n",
        "        \"\"\"\n",
        "        Compare responses side by side between models\n",
        "\n",
        "        Args:\n",
        "            models: List of models to compare. If None, compares all available.\n",
        "        \"\"\"\n",
        "        assessment_data = self.results.get(\"assessment_summary\", {})\n",
        "\n",
        "        if models is None:\n",
        "            models = list(assessment_data.keys())\n",
        "\n",
        "        print(\"ðŸ”„ RESPONSE COMPARISON\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Get best response from each model\n",
        "        model_best = {}\n",
        "        for model in models:\n",
        "            if model in assessment_data:\n",
        "                successful_results = [r for r in assessment_data[model].get(\"individual_results\", [])\n",
        "                                    if \"error\" not in r]\n",
        "                if successful_results:\n",
        "                    # Get highest scoring response\n",
        "                    best_response = max(successful_results, key=lambda x: x.get('overall_score', 0))\n",
        "                    model_best[model] = best_response\n",
        "\n",
        "        if len(model_best) < 2:\n",
        "            print(\"âŒ Need at least 2 models with successful results for comparison\")\n",
        "            return\n",
        "\n",
        "        # Display comparison\n",
        "        for model, response in model_best.items():\n",
        "            print(f\"\\nðŸ† BEST FROM {model.upper()}\")\n",
        "            print(f\"Score: {response.get('overall_score')}% | Words: {response.get('structure_analysis', {}).get('word_count', 'N/A')}\")\n",
        "            print(\"â”€\" * 50)\n",
        "\n",
        "            # Show first 300 characters of content\n",
        "            content = response.get('content', '')\n",
        "            preview = content[:300] + \"...\" if len(content) > 300 else content\n",
        "            print(self._format_content(preview))\n",
        "            print()\n",
        "\n",
        "    def analyze_patterns(self) -> None:\n",
        "        \"\"\"Analyze patterns across all responses\"\"\"\n",
        "\n",
        "        print(\"ðŸ§  PATTERN ANALYSIS\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        all_responses = []\n",
        "        assessment_data = self.results.get(\"assessment_summary\", {})\n",
        "\n",
        "        # Collect all successful responses\n",
        "        for model_data in assessment_data.values():\n",
        "            successful_results = [r for r in model_data.get(\"individual_results\", [])\n",
        "                                if \"error\" not in r]\n",
        "            all_responses.extend(successful_results)\n",
        "\n",
        "        if not all_responses:\n",
        "            print(\"âŒ No successful responses to analyze\")\n",
        "            return\n",
        "\n",
        "        # Analyze score distribution\n",
        "        scores = [r.get('overall_score', 0) for r in all_responses]\n",
        "        print(f\"ðŸ“Š Score Distribution:\")\n",
        "        print(f\"   Average: {sum(scores)/len(scores):.1f}%\")\n",
        "        print(f\"   Range: {min(scores):.1f}% - {max(scores):.1f}%\")\n",
        "        print(f\"   Excellent (â‰¥85%): {sum(1 for s in scores if s >= 85)}/{len(scores)} ({sum(1 for s in scores if s >= 85)/len(scores)*100:.0f}%)\")\n",
        "\n",
        "        # Analyze common terms found\n",
        "        print(f\"\\nðŸ” Common Successfully Detected Terms:\")\n",
        "        term_frequency = {}\n",
        "\n",
        "        for response in all_responses:\n",
        "            diagnostic_info = response.get('diagnostic_info', {})\n",
        "            found_terms = diagnostic_info.get('found_terms_by_category', {})\n",
        "\n",
        "            for category_terms in found_terms.values():\n",
        "                for term in category_terms:\n",
        "                    term_frequency[term] = term_frequency.get(term, 0) + 1\n",
        "\n",
        "        # Show top 10 most frequent terms\n",
        "        top_terms = sorted(term_frequency.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "        for term, freq in top_terms:\n",
        "            print(f\"   â€¢ {term}: {freq}/{len(all_responses)} responses ({freq/len(all_responses)*100:.0f}%)\")\n",
        "\n",
        "        # Analyze structure patterns\n",
        "        print(f\"\\nðŸ“‹ Structure Patterns:\")\n",
        "        numbered_items = [r.get('structure_analysis', {}).get('numbered_items', 0) for r in all_responses]\n",
        "        word_counts = [r.get('structure_analysis', {}).get('word_count', 0) for r in all_responses]\n",
        "\n",
        "        print(f\"   Average numbered items: {sum(numbered_items)/len(numbered_items):.1f}\")\n",
        "        print(f\"   Average word count: {sum(word_counts)/len(word_counts):.0f}\")\n",
        "        print(f\"   Perfect structure (5 points): {sum(1 for n in numbered_items if n == 5)}/{len(numbered_items)} ({sum(1 for n in numbered_items if n == 5)/len(numbered_items)*100:.0f}%)\")\n",
        "\n",
        "    def export_samples(self, output_file: str = None, format: str = \"markdown\") -> str:\n",
        "        \"\"\"\n",
        "        Export sample responses to file\n",
        "\n",
        "        Args:\n",
        "            output_file: Output filename. If None, generates timestamp-based name.\n",
        "            format: Export format ('markdown', 'txt', 'html')\n",
        "\n",
        "        Returns:\n",
        "            Path to exported file\n",
        "        \"\"\"\n",
        "        if output_file is None:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            extension = \"md\" if format == \"markdown\" else format\n",
        "            output_file = f\"vat_response_samples_{timestamp}.{extension}\"\n",
        "\n",
        "        assessment_data = self.results.get(\"assessment_summary\", {})\n",
        "\n",
        "        if format == \"markdown\":\n",
        "            content = self._generate_markdown_export(assessment_data)\n",
        "        elif format == \"txt\":\n",
        "            content = self._generate_text_export(assessment_data)\n",
        "        elif format == \"html\":\n",
        "            content = self._generate_html_export(assessment_data)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported format: {format}\")\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(content)\n",
        "\n",
        "        print(f\"ðŸ“ Samples exported to: {output_file}\")\n",
        "        return output_file\n",
        "\n",
        "    def _generate_markdown_export(self, assessment_data: Dict[str, Any]) -> str:\n",
        "        \"\"\"Generate markdown export of samples\"\"\"\n",
        "\n",
        "        content = [\"# Italian VAT Compliance - Generated Response Samples\\n\"]\n",
        "        content.append(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        content.append(f\"**Source:** {self.results_file}\\n\\n\")\n",
        "\n",
        "        for model, data in assessment_data.items():\n",
        "            successful_results = [r for r in data.get(\"individual_results\", []) if \"error\" not in r]\n",
        "\n",
        "            if not successful_results:\n",
        "                continue\n",
        "\n",
        "            content.append(f\"## {model.upper()} Samples\\n\")\n",
        "\n",
        "            # Show best response\n",
        "            best_response = max(successful_results, key=lambda x: x.get('overall_score', 0))\n",
        "\n",
        "            content.append(f\"**Best Performance:** {best_response.get('overall_score')}% ({best_response.get('quality_level')})\\n\")\n",
        "            content.append(f\"**Response Time:** {best_response.get('response_time')}s\\n\")\n",
        "            content.append(f\"**Word Count:** {best_response.get('structure_analysis', {}).get('word_count')}\\n\\n\")\n",
        "\n",
        "            content.append(\"### Generated Content:\\n\\n\")\n",
        "            content.append(\"```\\n\")\n",
        "            content.append(best_response.get('content', 'No content'))\n",
        "            content.append(\"\\n```\\n\\n\")\n",
        "\n",
        "            # Category breakdown\n",
        "            content.append(\"### Category Performance:\\n\\n\")\n",
        "            category_results = best_response.get('category_results', {})\n",
        "            for category, cat_data in category_results.items():\n",
        "                category_name = category.replace('_', ' ').title()\n",
        "                score = cat_data.get('category_score', 0)\n",
        "                matches = cat_data.get('matches', 0)\n",
        "                total = cat_data.get('total_terms', 0)\n",
        "                content.append(f\"- **{category_name}:** {score}% ({matches}/{total} terms)\\n\")\n",
        "\n",
        "            content.append(\"\\n\")\n",
        "\n",
        "        return ''.join(content)\n",
        "\n",
        "    def _generate_text_export(self, assessment_data: Dict[str, Any]) -> str:\n",
        "        \"\"\"Generate plain text export\"\"\"\n",
        "        lines = [\n",
        "            \"ITALIAN VAT COMPLIANCE - GENERATED RESPONSE SAMPLES\",\n",
        "            \"=\" * 60,\n",
        "            f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
        "            f\"Source: {self.results_file}\",\n",
        "            \"\"\n",
        "        ]\n",
        "\n",
        "        for model, data in assessment_data.items():\n",
        "            successful_results = [r for r in data.get(\"individual_results\", []) if \"error\" not in r]\n",
        "\n",
        "            if not successful_results:\n",
        "                continue\n",
        "\n",
        "            best_response = max(successful_results, key=lambda x: x.get('overall_score', 0))\n",
        "\n",
        "            lines.extend([\n",
        "                f\"{model.upper()} BEST SAMPLE\",\n",
        "                \"-\" * 30,\n",
        "                f\"Score: {best_response.get('overall_score')}%\",\n",
        "                f\"Quality: {best_response.get('quality_level')}\",\n",
        "                f\"Words: {best_response.get('structure_analysis', {}).get('word_count')}\",\n",
        "                \"\",\n",
        "                \"CONTENT:\",\n",
        "                best_response.get('content', 'No content'),\n",
        "                \"\",\n",
        "                \"=\" * 60,\n",
        "                \"\"\n",
        "            ])\n",
        "\n",
        "        return '\\n'.join(lines)\n",
        "\n",
        "    def _generate_html_export(self, assessment_data: Dict[str, Any]) -> str:\n",
        "        \"\"\"Generate HTML export\"\"\"\n",
        "        html_content = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>VAT Compliance Response Samples</title>\n",
        "    <style>\n",
        "        body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
        "        .sample {{ border: 1px solid #ddd; margin: 20px 0; padding: 20px; }}\n",
        "        .score {{ font-size: 18px; color: #2e8b57; font-weight: bold; }}\n",
        "        .content {{ background: #f5f5f5; padding: 15px; white-space: pre-wrap; }}\n",
        "        .metrics {{ display: grid; grid-template-columns: 1fr 1fr; gap: 10px; }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>ðŸ›ï¸ Italian VAT Compliance - Response Samples</h1>\n",
        "    <p><strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
        "    <p><strong>Source:</strong> {self.results_file}</p>\n",
        "\"\"\"\n",
        "\n",
        "        for model, data in assessment_data.items():\n",
        "            successful_results = [r for r in data.get(\"individual_results\", []) if \"error\" not in r]\n",
        "\n",
        "            if not successful_results:\n",
        "                continue\n",
        "\n",
        "            best_response = max(successful_results, key=lambda x: x.get('overall_score', 0))\n",
        "\n",
        "            html_content += f\"\"\"\n",
        "    <div class=\"sample\">\n",
        "        <h2>ðŸ¤– {model.upper()}</h2>\n",
        "        <div class=\"score\">Score: {best_response.get('overall_score')}% ({best_response.get('quality_level')})</div>\n",
        "        <div class=\"metrics\">\n",
        "            <div>Response Time: {best_response.get('response_time')}s</div>\n",
        "            <div>Word Count: {best_response.get('structure_analysis', {}).get('word_count')}</div>\n",
        "        </div>\n",
        "\n",
        "        <h3>Generated Content:</h3>\n",
        "        <div class=\"content\">{best_response.get('content', 'No content')}</div>\n",
        "    </div>\n",
        "\"\"\"\n",
        "\n",
        "        html_content += \"\"\"\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "        return html_content\n",
        "\n",
        "# Convenience functions for quick usage\n",
        "def view_samples(results_file: str = None, model: str = None, max_samples: int = 3):\n",
        "    \"\"\"Quick function to view samples\"\"\"\n",
        "    viewer = VATResultsViewer(results_file)\n",
        "    viewer.display_sample_responses(model, max_samples)\n",
        "\n",
        "def compare_models(results_file: str = None, models: List[str] = None):\n",
        "    \"\"\"Quick function to compare models\"\"\"\n",
        "    viewer = VATResultsViewer(results_file)\n",
        "    viewer.compare_responses(models)\n",
        "\n",
        "def analyze_all(results_file: str = None):\n",
        "    \"\"\"Quick function for full analysis\"\"\"\n",
        "    viewer = VATResultsViewer(results_file)\n",
        "    print(\"ðŸ“‹ Full Analysis Report\")\n",
        "    print(\"=\" * 50)\n",
        "    viewer.display_sample_responses(max_samples=2)\n",
        "    viewer.compare_responses()\n",
        "    viewer.analyze_patterns()\n",
        "\n",
        "def export_samples(results_file: str = None, format: str = \"markdown\"):\n",
        "    \"\"\"Quick function to export samples\"\"\"\n",
        "    viewer = VATResultsViewer(results_file)\n",
        "    return viewer.export_samples(format=format)\n",
        "\n",
        "# Main execution for testing\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ðŸ” VAT Results Viewer\")\n",
        "    print(\"Available functions:\")\n",
        "    print(\"- view_samples(): Display sample responses\")\n",
        "    print(\"- compare_models(): Compare model responses\")\n",
        "    print(\"- analyze_all(): Full analysis report\")\n",
        "    print(\"- export_samples(): Export to file\")\n",
        "\n",
        "    # Example usage\n",
        "    try:\n",
        "        viewer = VATResultsViewer()\n",
        "        print(f\"\\nLoaded results from: {viewer.results_file}\")\n",
        "\n",
        "        # Show quick sample\n",
        "        viewer.display_sample_responses(max_samples=1)\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"âŒ {e}\")\n",
        "        print(\"Run the diagnostic assessment first to generate results.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1sXaPMHN5S0",
        "outputId": "f3185df9-fa23-4ca4-e17a-853a1b54375f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” VAT Results Viewer\n",
            "Available functions:\n",
            "- view_samples(): Display sample responses\n",
            "- compare_models(): Compare model responses\n",
            "- analyze_all(): Full analysis report\n",
            "- export_samples(): Export to file\n",
            "\n",
            "Loaded results from: diagnostic_vat_assessment_20250625_095631.json\n",
            "ðŸ” SAMPLE GENERATED RESPONSES\n",
            "======================================================================\n",
            "\n",
            "ðŸ¤– GPT-3.5-TURBO - SAMPLE RESPONSES\n",
            "--------------------------------------------------\n",
            "\n",
            "ðŸ“„ SAMPLE 1 - gpt-3.5-turbo\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Score: 86.4% (Excellent)\n",
            "Response Time: 8.18s\n",
            "Word Count: 393\n",
            "Language: Italian\n",
            "Bonuses: Complete 5-point structure (+10%), Comprehensive content (+8%), Proper Italian language (+12%), Legal references present (1) (+4%)\n",
            "\n",
            "ðŸ“ GENERATED CONTENT:\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "   Certamente, ecco una panoramica dei 5 adempimenti IVA piÃ¹ critici per una SRL italiana in regime ordinario nel 2024:\n",
            "ðŸ”¹ 1. **Registrazione IVA e Comunicazioni Obbligatorie:**\n",
            "   - Requisito legale: Ogni SRL deve registrarsi presso l'Agenzia delle Entrate per l'IVA entro 30 giorni dall'inizio dell'attivitÃ  economica.\n",
            "   - Scadenze e frequenze: La registrazione Ã¨ valida fino a revoca e deve essere comunicata tramite il modello AA7.\n",
            "   - Conseguenze della non conformitÃ : La mancata registrazione comporta l'applicazione di sanzioni pecuniarie e l'impossibilitÃ  di detrarre l'IVA sugli acquisti.\n",
            "ðŸ”¹ 2. **Fatturazione Elettronica e Sistema di Interscambio (SdI):**\n",
            "   - Requisito legale: Le SRL devono emettere e ricevere solo fatture elettroniche conformi agli standard dell'Agenzia delle Entrate.\n",
            "   - Scadenze e frequenze: Le fatture devono essere trasmesse in tempo reale tramite il SdI entro 24 ore dalla loro emissione.\n",
            "   - Conseguenze della non conformitÃ : La mancata emissione di fatture elettroniche o errori nei dati possono comportare multe fino al 90% dell'importo non fatturato correttamente.\n",
            "ðŸ”¹ 3. **Liquidazione e Versamento dell'IVA:**\n",
            "   - Requisito legale: Le SRL devono presentare la liquidazione periodica dell'IVA entro il 16 del mese successivo al trimestre di riferimento.\n",
            "   - Scadenze e frequenze: Il versamento dell'IVA dovuta deve essere effettuato entro la stessa scadenza della liquidazione.\n",
            "   - Conseguenze della non conformitÃ : Il mancato versamento dell'IVA comporta l'applicazione di interessi di mora e sanzioni proporzionali all'importo non versato.\n",
            "ðŸ”¹ 4. **Dichiarazioni Fiscali Periodiche:**\n",
            "   - Requisito legale: Le SRL devono presentare la dichiarazione annuale IVA entro il 28 febbraio dell'anno successivo.\n",
            "   - Scadenze e frequenze: La dichiarazione annuale riepiloga l'IVA dovuta e detraibile nel corso dell'anno solare.\n",
            "   - Conseguenze della non conformitÃ : Il mancato invio della dichiarazione annuale puÃ² comportare l'applicazione di sanzioni e l'avvio di procedure di accertamento da parte dell'Agenzia delle Entrate.\n",
            "ðŸ”¹ 5. **Tenuta dei Registri Contabili e Conservazione Documentazione:**\n",
            "   - Requisito legale: Le SRL devono mantenere i registri contabili e la documentazione relativa all'IVA per almeno 10 anni.\n",
            "   - Scadenze e frequenze: La documentazione deve essere conservata in modo ordinato e accessibile per eventuali controlli.\n",
            "   - Conseguenze della non conformitÃ : La mancata conservazione dei documenti contabili puÃ² portare a sanzioni e alla perdita della possibilitÃ  di difendersi in caso di contestazioni da parte dell'Agenzia delle Entrate.\n",
            "   Ti consiglio di consultare il DPR 633/72 e successive modifiche, nonchÃ© le circolari dell'Agenzia delle Entrate per ulteriori dettagli e precisazioni in merito agli adempimenti IVA per le SRL italiane in regime ordinario nel 2024.\n",
            "\n",
            "ðŸ“Š CATEGORY PERFORMANCE:\n",
            "  â€¢ Compliance Terms: 34.8% (8/23 terms)\n",
            "  â€¢ Technical Terms: 77.3% (17/22 terms)\n",
            "  â€¢ Legal Refs: 8.3% (1/12 terms)\n",
            "  â€¢ Precision Terms: 52.9% (9/17 terms)\n",
            "  â€¢ Accuracy Terms: 77.8% (7/9 terms)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ðŸ¤– GPT-4 - SAMPLE RESPONSES\n",
            "--------------------------------------------------\n",
            "\n",
            "ðŸ“„ SAMPLE 1 - gpt-4\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "Score: 85.4% (Excellent)\n",
            "Response Time: 41.56s\n",
            "Word Count: 438\n",
            "Language: Italian\n",
            "Bonuses: Complete 5-point structure (+10%), Comprehensive content (+8%), Proper Italian language (+12%), Strong legal references (16) (+8%), Good monetary precision (10) (+6%)\n",
            "\n",
            "ðŸ“ GENERATED CONTENT:\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ðŸ”¹ 1. Presentazione della Dichiarazione IVA annuale:\n",
            "   - Requisito legale: secondo l'art. 4 del DPR 633/72, le SRL sono tenute a presentare la dichiarazione IVA annuale, che riassume tutte le operazioni IVA effettuate durante l'anno fiscale.\n",
            "   - Scadenza e frequenza: la dichiarazione IVA annuale deve essere presentata entro il 30 aprile dell'anno successivo a quello di riferimento.\n",
            "   - Conseguenze della non conformitÃ : la mancata presentazione o la presentazione tardiva della dichiarazione IVA annuale comporta sanzioni amministrative pecuniarie dal 120% al 240% dell'imposta dovuta, come previsto dall'art. 5 del D.Lgs. 471/97.\n",
            "ðŸ”¹ 2. Emissione e conservazione delle fatture elettroniche:\n",
            "   - Requisito legale: secondo l'art. 21 del DPR 633/72 e l'art. 1, commi 209-213, legge 205/2017, le SRL sono tenute a emettere e conservare le fatture in formato elettronico attraverso il Sistema di Interscambio (SdI).\n",
            "   - Scadenza e frequenza: le fatture devono essere emesse entro 10 giorni dalla data di effettuazione dell'operazione e conservate per 10 anni.\n",
            "   - Conseguenze della non conformitÃ : la mancata emissione o conservazione delle fatture elettroniche comporta sanzioni amministrative pecuniarie dal 90% al 180% dell'imposta dovuta, come previsto dall'art. 6 del D.Lgs. 471/97.\n",
            "ðŸ”¹ 3. Liquidazione periodica dell'IVA:\n",
            "   - Requisito legale: secondo l'art. 19 del DPR 633/72, le SRL sono tenute a calcolare e versare periodicamente l'IVA dovuta, detraendo l'IVA a credito.\n",
            "   - Scadenza e frequenza: la liquidazione dell'IVA deve essere effettuata mensilmente o trimestralmente, a seconda del volume d'affari, e l'IVA dovuta deve essere versata entro il 16 del mese successivo o entro il 16 del mese successivo al trimestre di riferimento.\n",
            "   - Conseguenze della non conformitÃ : la mancata o tardiva liquidazione dell'IVA comporta sanzioni amministrative pecuniarie dal 90% al 180% dell'imposta dovuta, come previsto dall'art. 6 del D.Lgs. 471/97.\n",
            "ðŸ”¹ 4. Registrazione delle operazioni IVA:\n",
            "   - Requisito legale: secondo l'art. 23 del DPR 633/72, le SRL sono tenute a registrare tutte le operazioni soggette ad IVA nei registri IVA.\n",
            "   - Scadenza e frequenza: le operazioni IVA devono essere registrate entro il 15 del mese successivo a quello di effettuazione dell'operazione.\n",
            "   - Conseguenze della non conformitÃ : la mancata o tardiva registrazione delle operazioni IVA comporta sanzioni amministrative pecuniarie dal 90% al 180% dell'imposta dovuta, come previsto dall'art. 6 del D.Lgs. 471/97.\n",
            "ðŸ”¹ 5. Comunicazione delle operazioni IVA:\n",
            "   - Requisito legale: secondo l'art. 50 del DPR 633/72, le SRL sono tenute a comunicare all'Agenzia delle Entrate i dati delle operazioni IVA effettuate.\n",
            "   - Scadenza e frequenza: la comunicazione deve essere effettuata entro il 30 aprile dell'anno successivo a quello di riferimento.\n",
            "   - Conseguenze della non conformitÃ : la mancata o tardiva comunicazione delle operazioni IVA comporta sanzioni amministrative pecuniarie dal 90% al 180% dell'imposta dovuta, come previsto dall'art. 11 del D.Lgs. 471/97.\n",
            "\n",
            "ðŸ“Š CATEGORY PERFORMANCE:\n",
            "  â€¢ Compliance Terms: 17.4% (4/23 terms)\n",
            "  â€¢ Technical Terms: 77.3% (17/22 terms)\n",
            "  â€¢ Legal Refs: 25.0% (3/12 terms)\n",
            "  â€¢ Precision Terms: 52.9% (9/17 terms)\n",
            "  â€¢ Accuracy Terms: 11.1% (1/9 terms)\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}