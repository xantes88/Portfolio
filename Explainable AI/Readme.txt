ğŸ§  Interpretable Neural Network for Regulatory Compliance in Banking

This project explores how explainable AI (XAI) techniques can be integrated with neural networks to create transparent, trustworthy systems for highly regulated domains such as banking and finance. By training a CNN on the MNIST dataset and applying a suite of interpretability tools, we simulate a compliance-focused model audit pipeline.

ğŸ” Objective

To build a visual and interactive pipeline that:

Trains a convolutional neural network on digit classification

Applies LIME, Integrated Gradients, Occlusion, and Grad-CAM

Provides human-interpretable insights into model decisions

Demonstrates how interpretability can support compliance and model auditing

ğŸ§  Technologies Used

Category	Tools & Libraries
Framework	PyTorch, Torchvision
Explainability	Captum, LIME, TorchCAM, Grad-CAM
Visualization	Matplotlib, NumPy
Dataset	MNIST (converted to RGB for CNNs)
Model	Custom CNN + DenseNet121 (pretrained)

ğŸ’¡ Why This Project Matters

âœ… Business Value
Regulatory Readiness
Simulates how models can be audited in finance (e.g. credit scoring, fraud detection) using explainability techniques aligned with GDPR, ECB, and EU AI Act.

Model Transparency
LIME and Integrated Gradients allow analysts and auditors to verify if models are focusing on relevant input features, improving trust and accountability.

Explainability Skills
Demonstrates deep understanding of black-box model interpretation â€” increasingly in demand across regulated industries.

ğŸ“ Project Structure
â”œâ”€â”€ data/
â”‚   â””â”€â”€ MNIST RGB formatted
â”œâ”€â”€ models/
â”‚   â””â”€â”€ trained_classifier.pth
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ interpretability_pipeline.ipynb
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ visualization_helpers.py
â”œâ”€â”€ explainability/
â”‚   â””â”€â”€ gradcam_output/
â”‚   â””â”€â”€ lime_heatmaps/
â”‚   â””â”€â”€ integrated_gradients/
â””â”€â”€ README.md
ğŸ§ª Methodology Overview

1. Preprocessing

Convert MNIST grayscale images to RGB

Resize to 224Ã—224 for compatibility with pretrained DenseNet

2. Training
Train both a custom CNN and a transfer learning model (DenseNet121)

Evaluate performance on test set

3. Explainability

Technique	Purpose

Integrated Gradients	Attribute pixel importance per class

LIME	Perturb local pixels, visualize decision

Grad-CAM / TorchCAM	Visualize CNN attention maps

Occlusion	Test robustness to input masking

ğŸ“Š Results

ğŸ”¥ High interpretability of predictions on digits 0â€“9

ğŸ’¬ Clear explanation maps for correct and incorrect predictions

ğŸ§  Local and global insights into what features drive decisions

ğŸ§© Modular pipeline that supports rapid compliance testing

ğŸ”„ Next Steps
âœ… Add SHAP for global feature importance

âœ… Visualize decision boundaries with t-SNE

ğŸ”„ Integrate Streamlit for real-time interactive model audit

ğŸ”„ Use adversarial examples to test explanation stability

ğŸ Conclusion
This project provides a transparent and reproducible explainability pipeline for neural network interpretability. It highlights how data scientists and ML engineers can build responsible AI systems â€” especially critical in banking, insurance, healthcare, and other compliance-heavy industries.
