ğŸ§  Interpretable Neural Networks for Banking Compliance

A complete prototype of an explainable AI (XAI) pipeline using deep learning and state-of-the-art interpretability tools (LIME, Integrated Gradients, Grad-CAM) applied to digit classification. While trained on MNIST for demonstration, it simulates real-world banking scenarios where compliance and transparency are non-negotiable.

ğŸ” Problem

In high-stakes sectors like finance, neural networks cannot operate as black boxes. Regulatory compliance (e.g. GDPR, ECB, EBA) increasingly demands explainability of decisions made by AI systems â€” particularly those affecting credit, fraud, and risk.

This project explores how deep learning models can be interpreted using visual saliency techniques, helping banks:

Understand why a model made a decision

Detect bias or data drift

Build trust with regulators and stakeholders

ğŸ§‘â€ğŸ’¼ Simulated Client Context

Used by a European retail bank (~5M clients, â‚¬12B assets) to prototype explainable credit scoring modules for internal compliance audits.
The model is trained on digit classification but serves as a base for credit document verification and fraud detection interpretation modules.

ğŸ“ˆ Quantified Business Value

Reduce compliance audit costs by ~â‚¬250K/year by pre-screening AI outputs before human review

Accelerate model approval by 30â€“40% during regulatory checks through transparent visual explanations

Increase internal model adoption by >60% as domain experts gain confidence via interpretable saliency maps

ğŸ§ª Techniques Demonstrated

Technique	Purpose	Library
LIME	Local surrogate interpretability	lime
Integrated Gradients	Feature attribution via gradients	captum
Grad-CAM	Heatmaps for convolutional layers	torchcam
Occlusion	Pixel masking for importance testing	captum

ğŸ§  Model Architecture

A custom CNN trained on MNIST (digit classification) with the following specs:

Input: Grayscale â†’ RGB conversion â†’ Resized to 224x224

Architecture: 2 Conv2D layers â†’ ReLU â†’ MaxPooling â†’ Dense

Training: Adam optimizer, 1 epoch, batch size 32

Exported model: MNIST_classifier.pth

ğŸ¯ Use Cases (Banking Context)

Use Case	How It Applies
Credit document validation	Explain OCR-based classifications
Fraud detection	Visualize why a transaction is flagged
Loan default risk scoring	Justify decisions in model audits
KYC document classification	Interpret classification of scanned inputs

ğŸ§  What the Model Sees
Using LIME and Integrated Gradients, the system produces explanations such as:

ğŸ”¥ Heatmaps that highlight pixel regions contributing most to predictions

ğŸ¯ Class-specific saliency â€” e.g. the loop of a â€œ6â€ vs. the stem of a â€œ1â€

âš ï¸ Discrepancy detection when wrong decisions are driven by noise

ğŸ§© Explainability Layers
The system includes both local (instance-level) and global (class-wide) interpretability:

Local: Integrated Gradients, LIME, Occlusion

Global: Grad-CAM, activation maps, t-SNE projections, feature distribution

ğŸ› ï¸ Stack
Python, PyTorch

TorchVision (DenseNet), Captum, TorchCAM

LIME, Matplotlib, Seaborn

Colab, GPU-compatible

ğŸ“Œ Next Improvements

Replace MNIST with banking-related image/text datasets

Expand to SHAP, Anchors, and Counterfactuals

Serve explanations via REST API for business un
