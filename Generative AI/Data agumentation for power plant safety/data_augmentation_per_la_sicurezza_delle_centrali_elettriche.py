# -*- coding: utf-8 -*-
"""Data Augmentation per la sicurezza delle centrali elettriche

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15yaxxRrOP_0bT1rIPhRip7J7MSm70Iib
"""

!pip install diffusers

!pip install datasets

!pip install --upgrade transformers
!pip install --upgrade diffusers

!pip install gputil

!pip install --upgrade transformers

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/uclanlp/visualbert.git
# %cd visualbert
!pip install -r requirements.txt

import collections
from collections import defaultdict
import torch
import matplotlib.pyplot as plt
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import sentence_transformers
from sentence_transformers import util
from sentence_transformers import SentenceTransformer
from torch.cuda.amp import autocast
import json
from torch.optim import lr_scheduler
from sklearn.metrics import classification_report
import seaborn as sns
import pandas as pd
from sklearn.metrics import confusion_matrix
from collections import Counter
import torch
import gc
import random
from torch.utils.data import Dataset, DataLoader, random_split
from diffusers import StableDiffusionPipeline
from transformers import BlipProcessor, BlipForConditionalGeneration, GPTNeoForCausalLM, GPT2Tokenizer
from PIL import Image
from tqdm import tqdm
from difflib import SequenceMatcher
from torchvision import models, transforms, datasets
from torchvision.datasets import OxfordIIITPet
import numpy as np
import os
from transformers import BlipProcessor, BlipForConditionalGeneration, BertTokenizer, BertModel
from transformers import ViltProcessor, ViltForQuestionAnswering
import torch.nn.functional as F
import torch.nn as nn
import torch.optim as optim
import csv
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix
from transformers import ViTForImageClassification, ViTFeatureExtractor
from PIL import ImageFile
import GPUtil
from google.colab import drive
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
import shutil
from google.colab import files
import zipfile

!pip install efficientnet-pytorch

# Retrieve all available GPUs
gpus = GPUtil.getGPUs()

# Display the memory status of each GPU
for gpu in gpus:
    print(f"GPU: {gpu.name}")
    print(f"  Total memory: {gpu.memoryTotal}MB")
    print(f"  Memory used: {gpu.memoryUsed}MB")
    print(f"  Memory free: {gpu.memoryFree}MB")
    print(f"  Memory usage percentage: {gpu.memoryUtil * 100:.2f}%")
    print(f"  GPU utilization percentage: {gpu.load * 100:.2f}%")
    print(f"  Temperature: {gpu.temperature}°C")
    print("-" * 50)

# 1. verifying if cuda is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using Device: {device}")

# 2. Image conversion for capptiong by Blip
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Ridimensionamento delle immagini
    transforms.ToTensor(),          # Converte l'immagine in un tensore
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizzazione per ImageNet
])

# Load the dataset without transformations
trainval_dataset = datasets.OxfordIIITPet(root='./data', split='trainval', download=True, transform=None)
test_dataset = datasets.OxfordIIITPet(root='./data', split='test', download=True, transform=None)
dataset = trainval_dataset + test_dataset
print(f"Dataset successfully loaded: {len(dataset)} immagini.")

# 3. Initialize the BLIP model
print("Inizialization of BLIP...")
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)

# 4.  i create Function to generate captions in batches
def generate_captions_batch(images):
    pil_images = [transforms.ToPILImage()(image).convert("RGB") for image in images]
    inputs = processor(images=pil_images, return_tensors="pt", padding=True).to(device)
    out = blip_model.generate(**inputs)
    captions = [processor.decode(out[i], skip_special_tokens=True) for i in range(len(out))]
    return captions

# 5. creation of Collate_fn function to convert images into tensors and resize them
def collate_fn(batch):
    images, labels = zip(*batch)
    resize = transforms.Resize((224, 224))  # Ridimensiona tutte le immagini alla stessa dimensione
    images = [resize(image) for image in images]  # Applica il ridimensionamento
    images = [transforms.ToTensor()(image) for image in images]
    return torch.stack(images), torch.tensor(labels)

# 6. Preparation of Dataloader
print("Preparation of Dataloader...")
batch_size = 16
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

# 7. Creation of the main folder for saved images
output_images_dir = '/content/saved_images'
os.makedirs(output_images_dir, exist_ok=True)

# 8.  Creation of  Function to create a folder by breed and save the image in its original format
def save_image_by_breed(image, label_name, image_id):
    try:
        breed_dir = os.path.join(output_images_dir, label_name)  # Cartella per la razza
        os.makedirs(breed_dir, exist_ok=True)  # Crea la cartella per la razza se non esiste
        image_path = os.path.join(breed_dir, f"{image_id}_original.jpg")  # Salva l'immagine
        original_pil_image = image.convert("RGB")  # Immagine originale senza trasformazioni
        original_pil_image.save(image_path)
        print(f"Image saved: {image_path}")
        return image_path
    except Exception as e:
        print(f"Error saving image {image_id} for breed {label_name}: {e}")
        return None

# 9. Save the images and captions for the entire dataset
data_list = []  # Lista per salvare tutte le informazioni
processed_batches = 0
total_batches = len(data_loader)
successful_saves = 0
image_save_errors = 0

# Creation of Update  in order to save JSON file progressively
json_output_file = '/content/captions_dataset_blip.json'

# Creation of Load the JSON file if it exists for progressive update
if os.path.exists(json_output_file):
    with open(json_output_file, 'r') as json_file:
        data_list = json.load(json_file)  # Carica la lista di dati precedenti

for idx, (images, labels) in tqdm(enumerate(data_loader), desc="Generating captions", unit="batch", total=total_batches):
    images = images.to(device)
    captions_batch = generate_captions_batch(images)

    for i, caption in enumerate(captions_batch):
        image_id = idx * batch_size + i


        label = labels[i].item()
        label_name = trainval_dataset.classes[label]

        # Recupera l'immagine originale dal dataset senza trasformazioni
        original_image = dataset[idx * batch_size + i][0]

        # Salvataggio dell'immagine nel formato originale (senza trasformazioni)
        original_image_path = save_image_by_breed(original_image, label_name, image_id)

        if original_image_path:

            data_list.append({
                "image": original_image_path,   # Path of the saved image in original format
                "caption": caption,             # Generated caption
                "label": label_name             # Original breed label
            })
            successful_saves += 1
        else:
            image_save_errors += 1

    # Progressive save of the JSON file after each batch
    try:
        with open(json_output_file, 'w') as json_file:
            json.dump(data_list, json_file, indent=4)
        print(f"File JSON aggiornato: {json_output_file}.")
    except Exception as e:
        print(f"Errore nel salvataggio progressivo del file JSON: {e}")

    processed_batches += 1

# 10. Puttig in off  the BLIP model and free GPU memory
del blip_model
torch.cuda.empty_cache()
print("Off of Blip in order to Free GPU.")

# 11. report
print(f"\nProcess completed!")
print(f"Total batches processed: {processed_batches}/{total_batches}")
print(f"Total images saved successfully: {successful_saves}")
print(f"Error in saving images: {image_save_errors}")



# 12. Download the JSON file and images at the end
files.download(json_output_file)


shutil.make_archive(output_images_dir, 'zip', output_images_dir)
files.download(f"{output_images_dir}.zip")

# Retrieve all available GPUs
gpus = GPUtil.getGPUs()

# Display the memory status of each GPU
for gpu in gpus:
    print(f"GPU: {gpu.name}")
    print(f"  Total memory: {gpu.memoryTotal}MB")
    print(f"  Memory used: {gpu.memoryUsed}MB")
    print(f"  Memory free: {gpu.memoryFree}MB")
    print(f"  Memory usage percentage: {gpu.memoryUtil * 100:.2f}%")
    print(f"  GPU utilization percentage: {gpu.load * 100:.2f}%")
    print(f"  Temperature: {gpu.temperature}°C")
    print("-" * 50)

# Paths and configurations
SAVE_DIR = "/content/generated_images"
ORIGINAL_JSON_PATH = "/content/captions_dataset_blip.json"
AUGMENTED_JSON_PATH = "/content/augmented_captions_dataset.json"
NUM_IMAGES_PER_LABEL = 10  # I put the maximum number of every generated images for categories

"""For available GPUS and Computing Units and for availability of the models I put a generation of 10 images for every category of 37 of the entire dataset and using of models for gpt2 for texts and stable diffusion2.1 with inference steps of 50 and a guidance scale of 7.5 for a good mixture of precision and computing efficiency of generating images with techniques of mixed precisions"""

# I set the inizialition of all the models for beginning the generations of the images
gpt2_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
gpt2_model = GPT2LMHeadModel.from_pretrained("gpt2").to('cuda')
sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
stable_diff_pipe = StableDiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-1",
    torch_dtype=torch.float16
).to("cuda")

# Creation of  Function to generate caption with GPT-2
def generate_caption_with_gpt2(image_path, max_length=50):
    prompt = f"Describe this image in great detail: {image_path}"
    inputs = gpt2_tokenizer.encode(prompt, return_tensors="pt").to('cuda')
    attention_mask = torch.ones(inputs.shape, device='cuda')
    pad_token_id = gpt2_tokenizer.eos_token_id

    # I apply mixed precision for the caption generations
    with torch.amp.autocast('cuda'):
        outputs = gpt2_model.generate(
            inputs,
            attention_mask=attention_mask,
            max_length=max_length,
            num_return_sequences=1,
            no_repeat_ngram_size=2,
            pad_token_id=pad_token_id,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
            do_sample=True
        )
    return gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)

#  Function to calculate similarity between the prompt and the generated caption
def calculate_similarity(prompt, caption):
    # Mixed precision during similarity calculation
    with torch.amp.autocast('cuda'):
        prompt_embedding = sentence_model.encode(prompt, convert_to_tensor=True)
        caption_embedding = sentence_model.encode(caption, convert_to_tensor=True)
        similarity = util.pytorch_cos_sim(prompt_embedding, caption_embedding)
    return similarity.item()

# Function to generate the image using Stable Diffusion
def generate_image_with_diffusion(prompt, label, subfolder, image_name):
    detailed_prompt = f"A detailed image of a {label} ({prompt}), photorealistic and high quality."
    subfolder_path = os.path.join(SAVE_DIR, subfolder)

    if not os.path.exists(subfolder_path):
        os.makedirs(subfolder_path)

    image_path = os.path.join(subfolder_path, f"{image_name}_artificial.png")

      # Mixed precision during image generation
    with torch.amp.autocast('cuda'):
        image = stable_diff_pipe(detailed_prompt,
                                 num_inference_steps=50,
                                 guidance_scale=7.5).images[0]
    image.save(image_path)

    return image_path

# Function to update the JSON file with generated data
def update_json_with_generated_data(new_entry):
    # Load existing data if the JSON file exists
    if os.path.exists(AUGMENTED_JSON_PATH):
        with open(AUGMENTED_JSON_PATH, "r") as f:
            augmented_data = json.load(f)
    else:
        augmented_data = []


    augmented_data.append(new_entry)


    with open(AUGMENTED_JSON_PATH, "w") as f:
        json.dump(augmented_data, f, indent=4)

# Generating new data with progress tracking
print("Generating new data...")

if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)

# Initializion of the list for new data and a dictionary to track the number of images per breed
new_data = []
label_count = {}

# Loading of the original JSON file to copy the structure
with open(ORIGINAL_JSON_PATH, "r") as f:
    original_data = json.load(f)

# Usage of the reference images and update captions
for entry in original_data:
    label = entry["label"]

    # Checking how many images have already been generated for this breed
    if label not in label_count:
        label_count[label] = 0

    # If 10 images have already been generated for this breed, move to the next breed
    if label_count[label] >= NUM_IMAGES_PER_LABEL:
        continue

    image_path = entry["image"]  # Percorso dell'immagine esistente nel dataset originale

    # Creation of the prompt based on the breed (label) in the JSON file
    prompt = entry["caption"]  # Usa la didascalia originale come prompt (senza modifiche)

    # Print the prompt and label
    print(f"Generating image for label: {entry['label']}")
    print(f"Prompt used: {prompt}")

    # Images generation
    image_name = os.path.splitext(os.path.basename(image_path))[0]
    generated_image_path = generate_image_with_diffusion(prompt, entry["label"], entry["label"], image_name)

    # Generation of the caption for the images
    caption = generate_caption_with_gpt2(generated_image_path)

    # Calculation of the similarity between the original prompt and the generated caption
    similarity = calculate_similarity(prompt, caption)

    # If similarity is below the threshold, use the original caption as fallback
    if similarity < 0.7:
        caption = prompt

    # Add the image and updated caption to the list of new data
    new_entry = {
        "image": generated_image_path,  # Path to the generated image
        "caption": caption,   # Generated or updated caption
        "label": entry["label"],  # Breed or category of the animal
    }

    # Updating of the JSON file with the new data
    update_json_with_generated_data(new_entry)


    label_count[label] += 1


print(f"Total images and captions updated: {len(new_data)}")

print(f"Images and captions saved in {AUGMENTED_JSON_PATH}")

# Path to the ZIP file
zip_file_path = '/content/saved_images zippato.zip'

# Path to the destination folder for extraction
extract_folder = '/content/saved_images'

# Check if the destination folder exists, otherwise create it
if not os.path.exists(extract_folder):
    os.makedirs(extract_folder)

# Extract the content of the ZIP file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_folder)

print(f"Files successfully extracted to the folder {extract_folder}")

# Load the existing JSON file
json_file = '/content/captions_dataset_blip.json'  # Correct the file path by removing extra quotes

# Read the dataset from the JSON file
with open(json_file, 'r') as file:
    data_list = json.load(file)

# Create a dictionary to map labels to a numerical index
label_to_index = {}
current_index = 0

# Populate the dictionary with unique labels
for entry in data_list:
    label_name = entry['label']  # Label name
    if label_name not in label_to_index:
        label_to_index[label_name] = current_index
        current_index += 1

# Add the numerical index to each entry in the dataset
for entry in data_list:
    label_name = entry['label']
    entry['label_idx'] = label_to_index[label_name]  # Add the numerical index

# Save the updated JSON file
with open(json_file, 'w') as file:
    json.dump(data_list, file, indent=4)

# Print some control information
print(f"Number of unique labels: {len(label_to_index)}")
print(f"Label to numerical index mapping: {label_to_index}")

# Specifica il percorso del tuo file
file_path = '/content/captions_dataset_blip.json'

# Scarica il file
files.download(file_path)

# Definition of the device for processing (GPU or CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Utilizzando il dispositivo: {device}")

# Loading the JSON file
json_file = '/content/captions_dataset_blip.json'

with open(json_file, 'r') as f:
    data = json.load(f)

# Transformations for the images
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),  # Converte l'immagine in tensore
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizzazione per ResNet
])

# I create a class for  Custom dataset
class CustomImageDataset(Dataset):
    def __init__(self, data, transform=None):
        self.data = data
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        image_path = item["image"]
        label = item["label_idx"]


        image = Image.open(image_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image, label

# Dataset Creation
dataset = CustomImageDataset(data, transform=transform)

# Sizing for training, validation, and test (70%, 15%, 15%)
train_size = int(0.7 * len(dataset))
val_size = int(0.15 * len(dataset))
test_size = len(dataset) - train_size - val_size

# Dataset Splitting
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

# Batch Dataloaders
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Function to show the category distribution
def show_category_distribution(dataloader, label_map, set_name="Dataset"):
    label_counts = Counter()

    for images, labels in dataloader:
        label_counts.update(labels.cpu().numpy())  # Count the labels in the batch

    print(f"Category distribution in {set_name}:")
    for label_idx, count in label_counts.items():
        print(f"Category: {label_map[label_idx]} | Number of images: {count}")

# Create a dictionary that maps indices to labels
label_map = {idx: label for idx, label in enumerate(set(item['label'] for item in data))}

# Check the category distribution for each set
show_category_distribution(train_loader, label_map, "Training Set")
show_category_distribution(val_loader, label_map, "Validation Set")
show_category_distribution(test_loader, label_map, "Test Set")

# Check the overall sizes of the sets
print(f"Training Set Size: {len(train_loader.dataset)}")
print(f"Validation Set Size: {len(val_loader.dataset)}")
print(f"Test Set Size: {len(test_loader.dataset)}")

def check_tensor_values(dataloader):
    image_min = float('inf')
    image_max = float('-inf')
    label_min = float('inf')
    label_max = float('-inf')
    label_set = set()

    # Iteration on all batches
    for batch_idx, (images, labels) in enumerate(dataloader):
        # Verification of maximum and minimum values for images
        image_min = min(image_min, images.min().item())
        image_max = max(image_max, images.max().item())

        # Verification of maximum and minimum values for labels
        label_min = min(label_min, labels.min().item())
        label_max = max(label_max, labels.max().item())

        # Gathering of all labels
        label_set.update(labels.cpu().numpy())

    # Print the results
    print(f"Image values: Min = {image_min}, Max = {image_max}")
    print(f"Label values: Min = {label_min}, Max = {label_max}")
    print(f"Number of unique labels: {len(label_set)}")
    print(f"Unique labels: {sorted(label_set)}")

# Perform the check for the training set, validation set, and test set
print("Verification of values for the Training Set:")
check_tensor_values(train_loader)

print("\nVerification of values for the Validation Set:")
check_tensor_values(val_loader)

print("\nVerification of values for the Test Set:")
check_tensor_values(test_loader)

# Define the device for training (GPU or CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Utilizzando il dispositivo: {device}")

# Load the pre-trained ResNet50 model
model = models.resnet50(pretrained=True)

# Modify the last layer to fit all the 37 classes
model.fc = nn.Linear(model.fc.in_features, 37)

# Move the model to the device
model = model.to(device)

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)


# Early Stopping
class EarlyStopping:
    def __init__(self, patience=2, delta=0):
        self.patience = patience
        self.delta = delta
        self.best_loss = None
        self.counter = 0
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss + self.delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0
        return self.early_stop

early_stopping = EarlyStopping(patience=2, delta=0.01)

# Training function
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25):
    best_model_wts = model.state_dict()
    best_acc = 0.0
    best_val_loss = float('inf')

    for epoch in range(num_epochs):
        print(f"Epoch {epoch}/{num_epochs - 1}")
        print("-" * 10)


        model.train()
        running_loss = 0.0
        running_corrects = 0

        for inputs, labels in train_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)


            optimizer.zero_grad()


            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            # Calculation of loss and for hte beckprepagions
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # running loss
            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)

        epoch_loss = running_loss / len(train_loader.dataset)
        epoch_acc = running_corrects.double() / len(train_loader.dataset)

        print(f"Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}")

        # Part of the code for the evalutation fase
        model.eval()
        running_loss = 0.0
        running_corrects = 0

        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            with torch.no_grad():
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)

                loss = criterion(outputs, labels)
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

        epoch_loss = running_loss / len(val_loader.dataset)
        epoch_acc = running_corrects.double() / len(val_loader.dataset)

        print(f"Validation Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}")

        # Updating of the chscduler
        scheduler.step()

        # Early stopping
        if early_stopping(epoch_loss):
            print("Early stopping attivato")
            break

        # part for saving the best model
        if epoch_acc > best_acc:
            best_acc = epoch_acc
            best_val_loss = epoch_loss
            best_model_wts = model.state_dict()

        print()

    # Part for loading the best weights for the model
    model.load_state_dict(best_model_wts)
    return model

#Training of the model
num_epochs = 15
model_trained = train_model(
    model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=num_epochs
)

# Saving of the model
torch.save(model_trained.state_dict(), 'resnet50_trained_model.pth')

# Final evaluation
def evaluate_model(model, test_loader):
    model.eval()
    all_preds = []
    all_labels = []
    running_corrects = 0
    total = 0
    for inputs, labels in test_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        with torch.no_grad():
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

        all_preds.append(preds.cpu().numpy())
        all_labels.append(labels.cpu().numpy())
        running_corrects += torch.sum(preds == labels.data)
        total += labels.size(0)

    # Unification of all labels and predictions
    all_preds = np.concatenate(all_preds)
    all_labels = np.concatenate(all_labels)

    accuracy = running_corrects.double() / total
    print(f"Test Accuracy: {accuracy:.4f}")

    # Classificaiton report
    report = classification_report(all_labels, all_preds, target_names=[str(i) for i in range(37)], output_dict=True)

    # Convert the report into a pandas DataFrame and transpose it
    report_df = pd.DataFrame(report).transpose()
    print("Classification Report:\n", report_df)

    # Calculation of confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[str(i) for i in range(37)], yticklabels=[str(i) for i in range(37)])
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.title('Confusion Matrix')
    plt.show()

# Final Evaluation
evaluate_model(model_trained, test_loader)

# Path of the ZIP file and destination directory
zip_file_path = "/content/saved_images (2).zip"
extract_to_path = "/content/saved_images"

# Create the destination directory if it doesn't exist
os.makedirs(extract_to_path, exist_ok=True)

# Extract the ZIP file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_to_path)

print(f"Files successfully extracted to: {extract_to_path}")

# Extract generated_images.zip to /content/generated_images
with zipfile.ZipFile("/content/generated_images.zip", 'r') as zip_ref:
    zip_ref.extractall("/content/generated_images")

print("Extraction completed!")

# Load the existing JSON file
json_file = '/content/augmented_captions_dataset.json'

# Read the dataset from the JSON file
with open(json_file, 'r') as file:
    data_list = json.load(file)

# Create a dictionary to map labels to a numeric index
label_to_index = {}
current_index = 0

# Populate the dictionary with unique labels
for entry in data_list:
    label_name = entry['label']  # Label name
    if label_name not in label_to_index:
        label_to_index[label_name] = current_index
        current_index += 1

# Add the numeric index to each entry in the dataset
for entry in data_list:
    label_name = entry['label']
    entry['label_idx'] = label_to_index[label_name]  # Add the numeric index

# Save the updated JSON file
with open(json_file, 'w') as file:
    json.dump(data_list, file, indent=4)

# Print some control information
print(f"Number of unique labels: {len(label_to_index)}")
print(f"Labels to numeric index: {label_to_index}")

# Percorsi dei file di input
file1_path = "/content/augmented_captions_dataset.json"
file2_path = "/content/captions_dataset_blip.json"
output_path = "/content/overall_dataset_augmented.json"

# Carica il primo file JSON
with open(file1_path, 'r') as file1:
    data1 = json.load(file1)

# Carica il secondo file JSON
with open(file2_path, 'r') as file2:
    data2 = json.load(file2)

# Fondi i due dataset (puoi personalizzare la logica se necessario)
merged_data = data1 + data2 if isinstance(data1, list) and isinstance(data2, list) else {**data1, **data2}

# Salva il file JSON fuso
with open(output_path, 'w') as output_file:
    json.dump(merged_data, output_file, indent=4)

print(f"Dataset fuso salvato in: {output_path}")

# Percorso del file da scaricare
files.download("/content/overall_dataset_augmented.json")

# Define the device for processing (GPU or CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load the JSON file
json_file = '/content/overall_dataset_augmented.json'

with open(json_file, 'r') as f:
    data = json.load(f)

# Transformations for the images
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),  # Converts the image to tensor
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization for ResNet
])

# Custom Dataset
class CustomImageDataset(Dataset):
    def __init__(self, data, transform=None):
        self.data = data
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        image_path = item["image"]
        label = item["label_idx"]

        # Load image
        image = Image.open(image_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image, label

# Create the dataset
dataset = CustomImageDataset(data, transform=transform)

# Calculate the sizes for training, validation, and test sets (70%, 15%, 15%)
train_size = int(0.7 * len(dataset))
val_size = int(0.15 * len(dataset))
test_size = len(dataset) - train_size - val_size

# Split the dataset
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

# Create DataLoader for batches
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Function to show the distribution of categories
def show_category_distribution(dataloader, label_map, set_name="Dataset"):
    label_counts = Counter()

    for images, labels in dataloader:
        label_counts.update(labels.cpu().numpy())  # Count the labels in the batch

    print(f"Category distribution in {set_name}:")
    for label_idx, count in label_counts.items():
        print(f"Category: {label_map[label_idx]} | Number of images: {count}")

# Create a dictionary that maps indices to labels
label_map = {idx: label for idx, label in enumerate(set(item['label'] for item in data))}

# Verify the category distribution for each set
show_category_distribution(train_loader, label_map, "Training Set")
show_category_distribution(val_loader, label_map, "Validation Set")
show_category_distribution(test_loader, label_map, "Test Set")

# Verify the total sizes of the sets
print(f"Training Set size: {len(train_loader.dataset)}")
print(f"Validation Set size: {len(val_loader.dataset)}")
print(f"Test Set size: {len(test_loader.dataset)}")

def check_tensor_values(dataloader):
    image_min = float('inf')
    image_max = float('-inf')
    label_min = float('inf')
    label_max = float('-inf')
    label_set = set()

    # Iteration on all batches
    for batch_idx, (images, labels) in enumerate(dataloader):
        # Verification of maximum and minimum values for images
        image_min = min(image_min, images.min().item())
        image_max = max(image_max, images.max().item())

        # Verification of maximum and minimum values for labels
        label_min = min(label_min, labels.min().item())
        label_max = max(label_max, labels.max().item())

        # Gathering of all labels
        label_set.update(labels.cpu().numpy())

    # Print the results
    print(f"Image values: Min = {image_min}, Max = {image_max}")
    print(f"Label values: Min = {label_min}, Max = {label_max}")
    print(f"Number of unique labels: {len(label_set)}")
    print(f"Unique labels: {sorted(label_set)}")

# Perform the check for the training set, validation set, and test set
print("Verification of values for the Training Set:")
check_tensor_values(train_loader)

print("\nVerification of values for the Validation Set:")
check_tensor_values(val_loader)

print("\nVerification of values for the Test Set:")
check_tensor_values(test_loader)

# Define the device for training (GPU or CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Utilizzando il dispositivo: {device}")

# Load the pre-trained ResNet50 model
model = models.resnet50(pretrained=True)

# Modify the last layer to fit all the 37 classes
model.fc = nn.Linear(model.fc.in_features, 37)

# Move the model to the device
model = model.to(device)

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)


# Early Stopping
class EarlyStopping:
    def __init__(self, patience=2, delta=0):
        self.patience = patience
        self.delta = delta
        self.best_loss = None
        self.counter = 0
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss + self.delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0
        return self.early_stop

early_stopping = EarlyStopping(patience=2, delta=0.01)

# Training function
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25):
    best_model_wts = model.state_dict()
    best_acc = 0.0
    best_val_loss = float('inf')

    for epoch in range(num_epochs):
        print(f"Epoch {epoch}/{num_epochs - 1}")
        print("-" * 10)


        model.train()
        running_loss = 0.0
        running_corrects = 0

        for inputs, labels in train_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)


            optimizer.zero_grad()


            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            # Calculation of loss and for hte beckprepagions
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # running loss
            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)

        epoch_loss = running_loss / len(train_loader.dataset)
        epoch_acc = running_corrects.double() / len(train_loader.dataset)

        print(f"Training Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}")

        # Part of the code for the evalutation fase
        model.eval()
        running_loss = 0.0
        running_corrects = 0

        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            with torch.no_grad():
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)

                loss = criterion(outputs, labels)
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

        epoch_loss = running_loss / len(val_loader.dataset)
        epoch_acc = running_corrects.double() / len(val_loader.dataset)

        print(f"Validation Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}")

        # Updating of the chscduler
        scheduler.step()

        # Early stopping
        if early_stopping(epoch_loss):
            print("Early stopping attivato")
            break

        # part for saving the best model
        if epoch_acc > best_acc:
            best_acc = epoch_acc
            best_val_loss = epoch_loss
            best_model_wts = model.state_dict()

        print()

    # Part for loading the best weights for the model
    model.load_state_dict(best_model_wts)
    return model

#Training of the model
num_epochs = 15
model_trained = train_model(
    model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=num_epochs
)

# Saving of the model
torch.save(model_trained.state_dict(), 'resnet50_trained_model_agumented.pth')

# Final evaluation
def evaluate_model(model, test_loader):
    model.eval()
    all_preds = []
    all_labels = []
    running_corrects = 0
    total = 0
    for inputs, labels in test_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        with torch.no_grad():
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

        all_preds.append(preds.cpu().numpy())
        all_labels.append(labels.cpu().numpy())
        running_corrects += torch.sum(preds == labels.data)
        total += labels.size(0)

    # Unification of all labels and predictions
    all_preds = np.concatenate(all_preds)
    all_labels = np.concatenate(all_labels)

    accuracy = running_corrects.double() / total
    print(f"Test Accuracy: {accuracy:.4f}")

    # Classificaiton report
    report = classification_report(all_labels, all_preds, target_names=[str(i) for i in range(37)], output_dict=True)

    # Convert the report into a pandas DataFrame and transpose it
    report_df = pd.DataFrame(report).transpose()
    print("Classification Report:\n", report_df)

    # Calculation of confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[str(i) for i in range(37)], yticklabels=[str(i) for i in range(37)])
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.title('Confusion Matrix')
    plt.show()

# Final Evaluation
evaluate_model(model_trained, test_loader)

"""###CONCLUSIONS

Comparison of Results Between Original and Augmented Dataset
Model Accuracy:

Without artificially generated images: 62.28%
With artificially generated images: 72.55%
Improvement: +10.27%
General Classification Report Metrics:

Macro Avg (Precision):
Without: 0.696
With: 0.759
Macro Avg (Recall):
Without: 0.635
With: 0.720
Macro Avg (F1-Score):
Without: 0.631
With: 0.719
These improvements indicate that the model has gained better ability to distinguish between classes, leading to overall stronger performance.

Support Analysis:

The total number of test samples increased from 1103 to 1162 images, which may have contributed to stabilizing the model's performance.
Impact of Artificial Images
Increased Precision and Recall:
Artificial images provided better generalization, contributing to a more balanced performance across many classes.

Improved Classes:
Classes 0, 1, 9, 15, 22, and 36 show significant improvements in both recall and F1-score.

Negative Performance Trends:
Some classes experienced marginal or slight declines, but most showed a clear positive trend.

Conclusion
Integrating artificially generated images significantly improved the model's performance, suggesting that dataset augmentation through the use of artificial images helped enhance the model's ability to recognize and classify images more accurately in the critical context of the project.
"""